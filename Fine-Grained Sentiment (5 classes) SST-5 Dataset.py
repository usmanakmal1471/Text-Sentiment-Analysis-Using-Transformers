# -*- coding: utf-8 -*-
"""Fine-Grained Sentiment (5 classes) SST-5 Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17mTWZki019qHHPM_8pVV0tjp0qHyFhUD

# **Fine-Grained Sentiment Analysis (5 classes) SST-5 Dataset**

### Step 1: Install & Import Dependencies
"""

!pip install transformers datasets -q

# Core libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support,
    confusion_matrix, ConfusionMatrixDisplay
)

# PyTorch / Transformers
import torch
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    Trainer, TrainingArguments
)

"""### Step 2: Mount Google Drive & Load Data"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

file_path = "/content/drive/MyDrive/SST-5 Dataset (5 Classes).csv"
df = pd.read_csv(file_path, encoding='ISO-8859-1')
df = df[['Text', 'Sentiment']].dropna()
df.head()

"""### Step 3: Visualize Dataset Distribution"""

print(df['Sentiment'].value_counts())

plt.figure(figsize=(6,4))
sns.countplot(data=df, x='Sentiment')
plt.title("Sentiment Class Distribution")
plt.xlabel("Sentiments")
plt.ylabel("Total no. of Texts")
plt.grid(False)
plt.show()

"""### Step 4: Encode Labels"""

label_mapping = {
    "Very Negative": 0,
    "Negative": 1,
    "Neutral": 2,
    "Positive": 3,
    "Very Positive": 4
}

df['label'] = df['Sentiment'].map(label_mapping)

print("Label Mapping:")
for k, v in label_mapping.items():
    print(f"{k}: {v}")

"""### Step 5: Train-Test Split"""

train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['Text'].tolist(),
    df['label'].tolist(),
    test_size=0.2,
    random_state=42
)

"""### Step 6: Tokenization"""

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)

train_encodings = tokenizer(
    train_texts,
    truncation=True,
    padding='max_length',
    max_length=128,
    return_tensors="pt"
)

test_encodings = tokenizer(
    test_texts,
    truncation=True,
    padding='max_length',
    max_length=128,
    return_tensors="pt"
)

"""### Step 7: Dataset Class"""

class SentimentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: self.encodings[key][idx] for key in self.encodings}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

    def __len__(self):
        return len(self.labels)


train_dataset = SentimentDataset(train_encodings, train_labels)
test_dataset = SentimentDataset(test_encodings, test_labels)

"""### Step 8: Load Pretrained Model"""

model = AutoModelForSequenceClassification.from_pretrained(model_name)

"""### Step 9: Define Metrics Function"""

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    acc = accuracy_score(labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

"""### Step 10: Evaluate Without Fine-Tuning"""

baseline_args = TrainingArguments(
    output_dir="./results",
    per_device_eval_batch_size=32,
    no_cuda=False
)

baseline_trainer = Trainer(
    model=model,
    args=baseline_args,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

baseline_metrics = baseline_trainer.evaluate()

print("Performance without Fine-Tuning:")
print(f"Accuracy : {baseline_metrics['eval_accuracy'] * 100:.2f}%")
print(f"Precision: {baseline_metrics['eval_precision'] * 100:.2f}%")
print(f"Recall   : {baseline_metrics['eval_recall'] * 100:.2f}%")
print(f"F1 Score : {baseline_metrics['eval_f1'] * 100:.2f}%")

"""#### Step 10.1:Confusion Matrix (Without Fine-Tuning)"""

baseline_predictions = baseline_trainer.predict(test_dataset)
y_true_baseline = baseline_predictions.label_ids
y_pred_baseline = baseline_predictions.predictions.argmax(-1)

cm_baseline = confusion_matrix(y_true_baseline, y_pred_baseline)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_baseline, display_labels=list(label_mapping.keys()))

disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix - Before Fine-Tuning")
plt.grid(False)
plt.show()

"""### Step 11: Fine-Tune the Model"""

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    evaluation_strategy="epoch",
    save_strategy="no",
    logging_dir="./logs",
    logging_steps=10,
    logging_strategy="epoch",
    report_to="none",

    learning_rate=2e-5,
    weight_decay=0.01,
    warmup_steps=500,
    lr_scheduler_type="linear"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

trainer.train()

"""### Step 12: Evaluate After Fine-Tuning"""

final_metrics = trainer.evaluate()

print("Performance after Fine-Tuning:")
print(f"Accuracy : {final_metrics['eval_accuracy'] * 100:.2f}%")
print(f"Precision: {final_metrics['eval_precision'] * 100:.2f}%")
print(f"Recall   : {final_metrics['eval_recall'] * 100:.2f}%")
print(f"F1 Score : {final_metrics['eval_f1'] * 100:.2f}%")

"""#### Step 12.1: Confusion Matrix (After Fine-Tuning)"""

predictions = trainer.predict(test_dataset)
y_true = predictions.label_ids
y_pred = predictions.predictions.argmax(-1)

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(label_mapping.keys()))

disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix - After Fine-Tuning")
plt.grid(False)
plt.show()

"""### Step 13: Plot Training vs Validation Loss"""

train_logs = trainer.state.log_history
train_loss = [log['loss'] for log in train_logs if 'loss' in log and 'epoch' in log]
eval_loss = []
seen_epochs = set()

for log in train_logs:
    if 'eval_loss' in log and 'epoch' in log:
        epoch = log['epoch']
        if epoch not in seen_epochs:
            eval_loss.append(log['eval_loss'])
            seen_epochs.add(epoch)

min_len = min(len(train_loss), len(eval_loss))
train_loss = train_loss[:min_len]
eval_loss = eval_loss[:min_len]
epochs = range(1, min_len + 1)

plt.figure(figsize=(5, 4))
plt.plot(epochs, train_loss, label='Training Loss', marker='o')
plt.plot(epochs, eval_loss, label='Validation Loss', marker='s')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(False)
plt.show()